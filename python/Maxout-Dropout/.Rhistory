train <- read.csv('data/train_normalized.csv')
str(train)
train <- read.csv('data/train_normalized.csv')
str(train, l)
str(train)
train <- read.csv('data/train_normalized.csv')
str(train)
summary(train)
smp_size <- floor(0.8 * nrow(train))
set.seed(123)
install.packages("caret")
library(caret)
train_rows_numbers <- sample(seq_len(nrow(train)), size = smp_size)
train_normalized <- read.csv('data/train_normalized.csv')
train <- read.csv('data/train.csv')
train_All <- data.frame(Response=train$Response, train_normalized)
str(train_All)
smp_size <- floor(0.8 * nrow(train_All))
set.seed(123)
train_sample_rows_numbers <- sample(seq_len(nrow(train)), size = smp_size)
train_large  <- train.for.pylearn2[train_sample_rows_numbers, ]          # 80% of train data
train_large  <- train_All[train_sample_rows_numbers, ]          # 80% of train data
train_mini   <- train_All[sample(1:nrow(train_All), 1000), ]
valid_large  <- train_All[-train_sample_rows_numbers, ]         # rest data (20%)
valid_mini <- valid_large[sample(1:nrow(valid_large),1000),]
test_normalized  <- read.csv('data/test_normalized.csv')
write.csv(train_large, "01.pydata/train.csv",row.names=F)
write.csv(train_mini,  "01.pydata/train_mini.csv",row.names=F)
write.csv(valid_large, "01.pydata/valid.csv",row.names=F)
write.csv(valid_mini,  "01.pydata/valid_mini.csv",row.names=F)
